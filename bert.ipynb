{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJqA4ewBCkiF",
        "colab_type": "code",
        "outputId": "9f57acd8-fce1-4333-d7c8-131b1b9dcfe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/58/3d789b98923da6485f376be1e04d59ad7003a63bdb2b04b5eea7e02857e5/transformers-2.5.0-py3-none-any.whl (481kB)\n",
            "\r\u001b[K     |▊                               | 10kB 23.8MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 29.4MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 35.2MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40kB 39.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51kB 26.7MB/s eta 0:00:01\r\u001b[K     |████                            | 61kB 23.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 71kB 20.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 81kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 92kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 102kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 112kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 122kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 133kB 18.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 143kB 18.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 153kB 18.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 163kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 174kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 184kB 18.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 194kB 18.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 204kB 18.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 215kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 225kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 235kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 245kB 18.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 256kB 18.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 266kB 18.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 276kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 286kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 296kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 307kB 18.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 317kB 18.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 327kB 18.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 337kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 348kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 358kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 368kB 18.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 378kB 18.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 389kB 18.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 399kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 409kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 419kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 430kB 18.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 440kB 18.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 450kB 18.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 460kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 471kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 481kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 491kB 18.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 23.9MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 32.7MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 40.4MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 43.2MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 45.8MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 47.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 48.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 50.6MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 51.9MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 53.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 53.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 53.1MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 53.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 53.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 53.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 53.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/1d/ea7e2c628942e686595736f73678348272120d026b7acd54fe43e5211bb1/tokenizers-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 60.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=f73ffd2d8b185159a15ac7499695d4c2e487b7ae6b93013f422ad9596758c4c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.0 transformers-2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx0n9slOCYxO",
        "colab_type": "code",
        "outputId": "26ca32ab-d3dc-48fb-ec2e-edd6cd09b9f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "\n",
        "from transformers import BertModel, BertForMaskedLM, BertConfig, BertPreTrainedModel\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fWQ3n--DkVS",
        "colab_type": "code",
        "outputId": "10251e55-6679-4b30-8ff6-82a352dfadc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/projet-alexandrin/'  # your new root path\n",
        "\n",
        "sys.path.append(os.path.join(root_path, 'notebooks')) # for importing from utils.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAAW_reuPtWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = [txt for i, txt in enumerate(token_to_index)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09ksgYdBCYxo",
        "colab_type": "code",
        "outputId": "d2063031-decb-435b-d5a5-1914ab15b85b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "#data import\n",
        "with open(root_path+'data/corneille_racine_mm.txt', encoding = 'utf-8') as file:\n",
        "    data = file.read().lower()\n",
        "\n",
        "#Tokenization\n",
        "\n",
        "sequences = data.split('\\n')\n",
        "sequences = list(filter(lambda x: len(x)>5,sequences))\n",
        "\n",
        "#data import\n",
        "# sequences = []\n",
        "# with open(root_path+'data/corneille_racine_mm.txt', encoding = 'utf-8') as fin:\n",
        "#     for line in fin:\n",
        "#       sequences.append(line)\n",
        "\n",
        "all_tokens=['[CLS]', '[SEP]', '[MASK]']\n",
        "for seq in sequences:\n",
        "    for token in seq.split(' '):\n",
        "        all_tokens+=[token]\n",
        "token_to_index = dict(\n",
        "    [(txt, int(i)) for i, txt in enumerate(all_tokens)])\n",
        "token_to_index = dict(\n",
        "    [(txt, int(i)) for i, txt in enumerate(token_to_index)])\n",
        "index_to_token = dict(\n",
        "    (int(i), txt) for txt, i in token_to_index.items())\n",
        "\n",
        "vocab_size = len(token_to_index)\n",
        "print('vocab_size ',vocab_size)\n",
        "\n",
        "input_tokens = []\n",
        "output_tokens = []\n",
        "expected_tokens = []\n",
        "special_range = list(range(12))+[23]+list(range(12, 23))\n",
        "\n",
        "for i in range(0,len(sequences)-4,2):\n",
        "    for j in special_range:\n",
        "      # on prend les vers deux à deux\n",
        "      input_1 = ('[CLS] '+sequences[i]+' [SEP]').split(' ')\n",
        "      input_2 = (sequences[i+1]+' [SEP]').split(' ')\n",
        "      expected_output_1 = (sequences[i+2]+' [SEP]').split(' ')\n",
        "      expected_output_2 = (sequences[i+3]+' [SEP]').split(' ')\n",
        "      # the model will fill the input_3 and input_4 mask per mask starting from the left to the right\n",
        "\n",
        "      input_3 = ['[MASK]','[MASK]','[MASK]','[MASK]','[MASK]','[MASK]',\n",
        "                '[MASK]','[MASK]','[MASK]','[MASK]','[MASK]','[MASK]','[SEP]']\n",
        "      input_4 = ['[MASK]','[MASK]','[MASK]','[MASK]','[MASK]','[MASK]',\n",
        "                '[MASK]','[MASK]','[MASK]','[MASK]','[MASK]', expected_output_2[-2],'[SEP]']  # putting expected_output_1 to mimic real conditions\n",
        "      if j<12 or j==23:\n",
        "          input_3[:j] = expected_output_1[:j]\n",
        "          input_4[-2] = '[MASK]'\n",
        "      elif j>=12:\n",
        "          input_3 = expected_output_1\n",
        "          input_4[:j-12] = expected_output_2[:j-12]\n",
        "\n",
        "      if len(input_1) == 14 and len(input_2) == 13 and len(expected_output_1) == 13 and len(expected_output_2) == 13:\n",
        "        if j<12:\n",
        "            input_tokens.append(input_1+input_2+input_3+input_4)\n",
        "            output_tokens.append([expected_output_1[j]])\n",
        "        else:\n",
        "            input_tokens.append(input_1+input_2+input_3+input_4)\n",
        "            output_tokens.append([expected_output_2[j-12]])\n",
        "\n",
        "\n",
        "num_input = len(input_tokens)\n",
        "print('num_input ',num_input)\n",
        "\n",
        "max_seq_len = len(input_tokens[0])\n",
        "print('max_seq_len ',max_seq_len)\n",
        "\n",
        "input_ids = []\n",
        "output_ids = []\n",
        "\n",
        "for i in range(num_input):\n",
        "  tmp_input = []\n",
        "  for j in range(max_seq_len):\n",
        "      try:\n",
        "        tmp_input.append(token_to_index[input_tokens[i][j]])\n",
        "      except:\n",
        "        pass\n",
        "        # print(i,j)\n",
        "  input_ids.append(tmp_input)\n",
        "  output_ids.append(token_to_index[output_tokens[i][0]])\n",
        "\n",
        "print('Sequences')\n",
        "print(sequences[0:2])\n",
        "print('Tokens')\n",
        "print(input_tokens[0])\n",
        "print('Index')\n",
        "print(input_ids[0])\n",
        "\n",
        "#Attention Mask\n",
        "print('Attention MASK')\n",
        "attn_mask = []\n",
        "\n",
        "for token in input_tokens:\n",
        "    attn_mask.append([1 if tk != '[MASK]' else 0 for tk in token])\n",
        "print(attn_mask[0])\n",
        "\n",
        "#Segment Tokens\n",
        "print('Segment Tokens')\n",
        "seg_ids = []\n",
        "for token in input_tokens:\n",
        "    seg_ids.append([0 if i < 27 else 1 for i,tk in enumerate(token)])\n",
        "print(seg_ids[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size  3201\n",
            "num_input  949104\n",
            "max_seq_len  53\n",
            "Sequences\n",
            "['j@ t@ la vou a mi mon mal ai ltin ku rabl', 'j@ ni sai kun r@ maid e jan sywi lzin ka pabl']\n",
            "Tokens\n",
            "['[CLS]', 'j@', 't@', 'la', 'vou', 'a', 'mi', 'mon', 'mal', 'ai', 'ltin', 'ku', 'rabl', '[SEP]', 'j@', 'ni', 'sai', 'kun', 'r@', 'maid', 'e', 'jan', 'sywi', 'lzin', 'ka', 'pabl', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[SEP]']\n",
            "Index\n",
            "[0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1, 3, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1]\n",
            "Attention MASK\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "Segment Tokens\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N5YkqU_JiIC",
        "colab_type": "code",
        "outputId": "40b10470-b9ae-4134-8b4a-de90391deafb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "for i in range(4):\n",
        "  print(sequences[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "j@ t@ la vou a mi mon mal ai ltin ku rabl\n",
            "j@ ni sai kun r@ maid e jan sywi lzin ka pabl\n",
            "l@ shan j@@ s@ rai just a prai tan d@ ri goer\n",
            "mai mal gre se de din me lit aa tou mon koer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdVekSmE8ObJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = np.array(input_ids)\n",
        "attn_mask = np.array(attn_mask)\n",
        "seg_ids = np.array(seg_ids)\n",
        "output_ids = np.array(output_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCtZLobXCYxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 4\n",
        "num_heads = 4\n",
        "\n",
        "config = BertConfig(vocab_size=3201,\n",
        "        hidden_size=768,\n",
        "        num_hidden_layers=num_layers,\n",
        "        num_attention_heads=num_heads,\n",
        "        intermediate_size=3072,\n",
        "        hidden_act=\"gelu\",\n",
        "        hidden_dropout_prob=0.1,\n",
        "        attention_probs_dropout_prob=0.1,\n",
        "        max_position_embeddings=512,\n",
        "        type_vocab_size=2,\n",
        "        initializer_range=0.02,\n",
        "        layer_norm_eps=1e-12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9xz0Mvc7qb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy from https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.9, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2br5EupVrOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy from https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\" Original Implementation of the gelu activation function in Google Bert repo when initially created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "class BertPredictionHeadTransform(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.transform_act_fn = gelu\n",
        "        else:\n",
        "            self.transform_act_fn = config.hidden_act\n",
        "        self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLMPredictionHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.transform = BertPredictionHeadTransform(config)\n",
        "\n",
        "        # The output weights are the same as the input embeddings, but there is\n",
        "        # an output-only bias for each token.\n",
        "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "\n",
        "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
        "        self.decoder.bias = self.bias\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.decoder(hidden_states) + self.bias\n",
        "        return hidden_states\n",
        "\n",
        "class BertOnlyMLMHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.predictions = BertLMPredictionHead(config)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBeZ2HmGdyip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlexendrinePredictor(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        #Instantiating BERT model object \n",
        "        self.bert = BertModel(config)\n",
        "        #Classification layer\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "\n",
        "        #Feeding the input to BERT model to obtain contextualized representations\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.cls(sequence_output)\n",
        "\n",
        "        outputs = (prediction_scores,)# + outputs[2:]\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfVrJFuYqjAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = AlexendrinePredictor(config)\n",
        "# net.from_pretrained(root_path + 'save' + str(num_layers) + '/')  # does not load weights apparently!\n",
        "net.load_state_dict(torch.load(root_path + 'save' + str(num_layers) + '/pytorch_model.bin'))  # this seems to work\n",
        "net = net.cuda()\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbJvWpJYesoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opti = optim.Adam(net.parameters(), lr = 1e-6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLSXrndUTXjZ",
        "colab_type": "code",
        "outputId": "94003461-4e33-44cc-f9f1-0d5eeb8889fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCH = 3\n",
        "batch_size = 128\n",
        "temperature = 0.7  # cf post https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317#gistcomment-2918768\n",
        "\n",
        "for ep in range(EPOCH):\n",
        "    L = [] # We are going to average each loss all n batchs.\n",
        "    min_Loss = 10 # And Save the best average loss.\n",
        "    \n",
        "    permutation = torch.randperm(len(input_ids))      # Shuffle Data\n",
        "    print('Epoch {}/{}'.format(ep+1,EPOCH))\n",
        "    Max_ind = (len(input_ids)//batch_size)*batch_size # Avoid issues when len(input_ids) % batch_size != 0\n",
        "    start = time.time()\n",
        "    \n",
        "    for it in range(0, Max_ind, batch_size):\n",
        "        indices = permutation[it:it+batch_size]\n",
        "        \n",
        "        seq = input_ids[indices]\n",
        "        at_m = attn_mask[indices]\n",
        "        s_ids = seg_ids[indices]\n",
        "        lab = output_ids[indices]\n",
        "\n",
        "        first_mask_indices = np.argmax(seq == 2, axis = 1)  # first mask\n",
        "\n",
        "        seq_tensor = torch.tensor(seq).to(torch.long).cuda()\n",
        "        at_m_tensor = torch.tensor(at_m).cuda()\n",
        "        s_ids_tensor = torch.tensor(s_ids).cuda()\n",
        "        lab_tensor = torch.tensor(lab).cuda()\n",
        "        \n",
        "        #Clear gradients\n",
        "        opti.zero_grad()  \n",
        "\n",
        "        #Obtaining the logits from the model\n",
        "        outputs = net(input_ids = seq_tensor,\n",
        "                      attention_mask = at_m_tensor,\n",
        "                      token_type_ids = s_ids_tensor)\n",
        "        \n",
        "        logits = torch.cat([torch.unsqueeze(outputs[0][i][first_mask_indices[i]], 0) for i in range(batch_size)], axis=0).cuda()\n",
        "\n",
        "        # logits = torch.cat([torch.unsqueeze(top_k_top_p_filtering(outputs[0][i][first_mask_indices[i]] / temperature, filter_value=-10000), 0) for i in range(batch_size)], axis=0)\n",
        "\n",
        "        #Computing loss\n",
        "        loss = criterion(logits, lab_tensor.long())\n",
        "\n",
        "        #Backpropagating the gradients\n",
        "        loss.backward()\n",
        "        L.append(loss.item())\n",
        "\n",
        "        #Optimization step\n",
        "        opti.step()\n",
        "\n",
        "        if (it//batch_size + 1) % 100 == 0:\n",
        "            print(\"  Iteration {}/{} done. Average loss: {:.4f}. Time: {} secs\".format(it//batch_size+1, len(input_ids)//batch_size + 1, np.mean(L), int(time.time() - start)))\n",
        "            start = time.time()\n",
        "            L = []\n",
        "            \n",
        "        if (it//batch_size + 1) % 500 == 0:\n",
        "            net.save_pretrained(root_path + 'save' + str(num_layers) + '/')\n",
        "            print(\"Model saved.\")\n",
        "            # if np.mean(L)<min_Loss:\n",
        "            #   net.save_pretrained(root_path + '/save' + str(num_layers) + '/')\n",
        "            #   min_Loss = np.mean(L)\n",
        "            #   L=[]\n",
        "\n",
        "net.save_pretrained(root_path + 'save' + str(num_layers) + '/')\n",
        "print(\"Model saved.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "  Iteration 100/7415 done. Average loss: 4.0155. Time: 29 secs\n",
            "  Iteration 200/7415 done. Average loss: 3.9949. Time: 29 secs\n",
            "  Iteration 300/7415 done. Average loss: 3.9765. Time: 29 secs\n",
            "  Iteration 400/7415 done. Average loss: 3.9971. Time: 29 secs\n",
            "  Iteration 500/7415 done. Average loss: 4.0213. Time: 29 secs\n",
            "Model saved.\n",
            "  Iteration 600/7415 done. Average loss: 3.9986. Time: 29 secs\n",
            "  Iteration 700/7415 done. Average loss: 3.9914. Time: 29 secs\n",
            "  Iteration 800/7415 done. Average loss: 3.9763. Time: 29 secs\n",
            "  Iteration 900/7415 done. Average loss: 4.0042. Time: 29 secs\n",
            "  Iteration 1000/7415 done. Average loss: 3.9823. Time: 29 secs\n",
            "Model saved.\n",
            "  Iteration 1100/7415 done. Average loss: 4.0127. Time: 29 secs\n",
            "  Iteration 1200/7415 done. Average loss: 4.0058. Time: 29 secs\n",
            "  Iteration 1300/7415 done. Average loss: 3.9966. Time: 29 secs\n",
            "  Iteration 1400/7415 done. Average loss: 4.0005. Time: 29 secs\n",
            "  Iteration 1500/7415 done. Average loss: 4.0021. Time: 29 secs\n",
            "Model saved.\n",
            "  Iteration 1600/7415 done. Average loss: 4.0057. Time: 29 secs\n",
            "  Iteration 1700/7415 done. Average loss: 4.0042. Time: 29 secs\n",
            "  Iteration 1800/7415 done. Average loss: 4.0093. Time: 28 secs\n",
            "  Iteration 1900/7415 done. Average loss: 3.9551. Time: 29 secs\n",
            "  Iteration 2000/7415 done. Average loss: 3.9989. Time: 28 secs\n",
            "Model saved.\n",
            "  Iteration 2100/7415 done. Average loss: 3.9876. Time: 29 secs\n",
            "  Iteration 2200/7415 done. Average loss: 4.0311. Time: 29 secs\n",
            "  Iteration 2300/7415 done. Average loss: 3.9677. Time: 29 secs\n",
            "  Iteration 2400/7415 done. Average loss: 3.9745. Time: 29 secs\n",
            "  Iteration 2500/7415 done. Average loss: 3.9789. Time: 29 secs\n",
            "Model saved.\n",
            "  Iteration 2600/7415 done. Average loss: 3.9904. Time: 29 secs\n",
            "  Iteration 2700/7415 done. Average loss: 4.0363. Time: 29 secs\n",
            "  Iteration 2800/7415 done. Average loss: 3.9942. Time: 29 secs\n",
            "  Iteration 2900/7415 done. Average loss: 3.9620. Time: 29 secs\n",
            "  Iteration 3000/7415 done. Average loss: 3.9898. Time: 29 secs\n",
            "Model saved.\n",
            "  Iteration 3100/7415 done. Average loss: 4.0001. Time: 29 secs\n",
            "  Iteration 3200/7415 done. Average loss: 3.9534. Time: 29 secs\n",
            "  Iteration 3300/7415 done. Average loss: 3.9659. Time: 29 secs\n",
            "  Iteration 3400/7415 done. Average loss: 4.0055. Time: 29 secs\n",
            "  Iteration 3500/7415 done. Average loss: 3.9868. Time: 29 secs\n",
            "Model saved.\n",
            "  Iteration 3600/7415 done. Average loss: 3.9818. Time: 29 secs\n",
            "  Iteration 3700/7415 done. Average loss: 4.0278. Time: 29 secs\n",
            "  Iteration 3800/7415 done. Average loss: 3.9764. Time: 29 secs\n",
            "  Iteration 3900/7415 done. Average loss: 3.9669. Time: 29 secs\n",
            "  Iteration 4000/7415 done. Average loss: 3.9872. Time: 29 secs\n",
            "Model saved.\n",
            "  Iteration 4100/7415 done. Average loss: 4.0103. Time: 29 secs\n",
            "  Iteration 4200/7415 done. Average loss: 3.9708. Time: 29 secs\n",
            "  Iteration 4300/7415 done. Average loss: 3.9839. Time: 29 secs\n",
            "  Iteration 4400/7415 done. Average loss: 3.9743. Time: 29 secs\n",
            "  Iteration 4500/7415 done. Average loss: 3.9866. Time: 29 secs\n",
            "Model saved.\n",
            "  Iteration 4600/7415 done. Average loss: 3.9757. Time: 29 secs\n",
            "  Iteration 4700/7415 done. Average loss: 3.9845. Time: 29 secs\n",
            "  Iteration 4800/7415 done. Average loss: 3.9779. Time: 29 secs\n",
            "  Iteration 4900/7415 done. Average loss: 3.9793. Time: 29 secs\n",
            "  Iteration 5000/7415 done. Average loss: 3.9733. Time: 29 secs\n",
            "Model saved.\n",
            "  Iteration 5100/7415 done. Average loss: 3.9641. Time: 29 secs\n",
            "  Iteration 5200/7415 done. Average loss: 4.0223. Time: 29 secs\n",
            "  Iteration 5300/7415 done. Average loss: 3.9452. Time: 29 secs\n",
            "  Iteration 5400/7415 done. Average loss: 3.9703. Time: 29 secs\n",
            "  Iteration 5500/7415 done. Average loss: 3.9820. Time: 29 secs\n",
            "Model saved.\n",
            "  Iteration 5600/7415 done. Average loss: 3.9900. Time: 29 secs\n",
            "  Iteration 5700/7415 done. Average loss: 3.9587. Time: 29 secs\n",
            "  Iteration 5800/7415 done. Average loss: 4.0111. Time: 29 secs\n",
            "  Iteration 5900/7415 done. Average loss: 3.9872. Time: 29 secs\n",
            "  Iteration 6000/7415 done. Average loss: 3.9605. Time: 29 secs\n",
            "Model saved.\n",
            "  Iteration 6100/7415 done. Average loss: 3.9772. Time: 29 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBwe7tJ3R_7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_verses(verse1, verse2, temperature = 0.7, top_k = 0, top_p = 0.9):\n",
        "    \"\"\"input should be strings in metrometre format\"\"\"\n",
        "    \n",
        "    input_tokens = []\n",
        "    output_tokens = []\n",
        "    expected_tokens = []\n",
        "    \n",
        "    special_range = list(range(12))+[23]+list(range(12, 23))\n",
        "\n",
        "    input_1 = ('[CLS] '+ verse1 +' [SEP]').split(' ')\n",
        "    input_2 = (verse2 +' [SEP]').split(' ')\n",
        "    assert len(input_1) == 14 and len(input_2) == 13, \"verses should be alexandrines\"\n",
        "    input_3 = ['[MASK]','[MASK]','[MASK]','[MASK]','[MASK]','[MASK]',\n",
        "                '[MASK]','[MASK]','[MASK]','[MASK]','[MASK]','[MASK]','[SEP]']\n",
        "    input_4 = ['[MASK]','[MASK]','[MASK]','[MASK]','[MASK]','[MASK]',\n",
        "                '[MASK]','[MASK]','[MASK]','[MASK]','[MASK]','[MASK]', '[SEP]'] \n",
        "    \n",
        "    input_tokens = input_1 + input_2 + input_3 + input_4\n",
        "\n",
        "    input_ids = np.array([token_to_index[input_tokens[j]] for j in range(len(input_tokens))])\n",
        "        \n",
        "    seg_ids = [int(i >= 27)for i in range(max_seq_len)]\n",
        "    \n",
        "    for i in special_range:\n",
        "        attn_mask = [1 if tk != 2 else 0 for tk in input_ids]\n",
        "\n",
        "        seq_tensor = torch.tensor(input_ids).unsqueeze(0).cuda()\n",
        "        at_m_tensor = torch.tensor(attn_mask).unsqueeze(0).cuda()\n",
        "        s_ids_tensor = torch.tensor(seg_ids).unsqueeze(0).cuda()\n",
        "\n",
        "        first_mask_idx = np.argmax(input_ids == 2)  # first mask\n",
        "\n",
        "        outputs = net(input_ids = seq_tensor,\n",
        "                      attention_mask = at_m_tensor,\n",
        "                      token_type_ids = s_ids_tensor)\n",
        "\n",
        "        if i != 23:\n",
        "            # Keep only the last token predictions of the first batch item (batch size 1), apply a temperature coefficient and filter\n",
        "            logits = outputs[0][0][first_mask_idx] / temperature\n",
        "            filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "            # Sample from the filtered distribution\n",
        "            probabilities = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
        "            id_pred = torch.multinomial(probabilities, 1).item()\n",
        "            input_ids[first_mask_idx] = id_pred\n",
        "        else:\n",
        "            id_pred = outputs[0][0][first_mask_idx].max(0)[1].item()  # we want the rhyme\n",
        "            input_ids[-2] = id_pred\n",
        "            \n",
        "    \n",
        "    verse3 = ' '.join([index_to_token[input_ids[j]] for j in range(27, 39)])\n",
        "    verse4 = ' '.join([index_to_token[input_ids[j]] for j in range(40, 52)])\n",
        "    \n",
        "    return verse3, verse4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBBiW9pN9-q3",
        "colab_type": "code",
        "outputId": "caa58c9f-7dda-4bb3-ea43-c685ef501f64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "import transformer_fr_to_mm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 10541\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 5270\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 2635\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 1317\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 658\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 329\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 164\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 82\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 41\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 20\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 10\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 15\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 12\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 11\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 57271\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 28635\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 14317\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 7158\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 3579\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 1789\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 894\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 447\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 223\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 111\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 55\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 27\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 13\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 6\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 3\n",
            "INFO:absl:SubwordTextEncoder build: trying min_token_count 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8038 4612\n",
            "2to1: Latest checkpoint restored!!\n",
            "tensorflow_to_mm: loaded everything\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pvOuQNd10J4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(verse, top_p=0.9, temp=0.7, verbose=True):\n",
        "  return transformer_fr_to_mm.translate(verse, top_p=top_p, temp=temp, verbose=verbose, direction_1to2=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL9339LkrbtR",
        "colab_type": "code",
        "outputId": "d10ef7fa-8b3e-4f09-fc00-a6bc84e6f3c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "source": [
        "temp = 0.7\n",
        "top_k = 0\n",
        "top_p = 0.9\n",
        "temp2 = 0.6\n",
        "top_k2 = 1\n",
        "top_p2 = 0.9\n",
        "\n",
        "for i in range(5):\n",
        "  idx = np.random.randint(len(sequences)//2)\n",
        "  print(idx)\n",
        "  verse1 = sequences[2*idx]\n",
        "  verse2 = sequences[2*idx+1]\n",
        "  verse3, verse4 = generate_verses(verse1, verse2, temperature=temp, top_k=top_k, top_p=top_p)\n",
        "  print(\"Input 1:  {}\".format(verse1))\n",
        "  print(\"Input 2:  {}\".format(verse2))\n",
        "  print(\"Output 1: {}\".format(verse3))  \n",
        "  print(\"Output 2: {}\".format(verse4))\n",
        "  print(\"Traduction Output 1: {}\".format(translate(verse3, top_p=top_p2, temp=temp2, verbose=False)))\n",
        "  print(\"Traduction Output 2: {}\".format(translate(verse4, top_p=top_p2, temp=temp2, verbose=False)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10903\n",
            "Input 1:  j@ veu tan te pour tanlt an kor un oo tr@ vwa\n",
            "Input 2:  a m@@ ne po li eukt e si j@ l@ ran vwa\n",
            "Output 1: an fin j@ nai pwin just e d@ tou t@@ la vi\n",
            "Output 2: a vaik d@ la r@ grai di ny@ d@ me r@ vi\n",
            "Traduction Output 1: enfin je n ai point juste et de toute la vie\n",
            "Traduction Output 2: avec de la regret digne de mes \n",
            "15631\n",
            "Input 1:  e ki m@ port an kor d@ plulz e tran j@@ kou\n",
            "Input 2:  vwa si sai san rai zon k@ jan lne tai ja lou\n",
            "Output 1: e kan ltil s@ rai zonln a prai s@ gran sou pir\n",
            "Output 2: si vo tr@ koer an kor a vai ra de kon dir\n",
            "Traduction Output 1: et quand il se raison après ce grand soupir\n",
            "Traduction Output 2: si votre coeur encore avait ravi des dire\n",
            "12174\n",
            "Input 1:  l@ myinln ai san pa raiy pywis k@ j@ vou lzan bras\n",
            "Input 2:  jai ron pu voo dis kour da se moo vai z@@ graas\n",
            "Output 1: sai ltin so ra m@@ jour lza van je la dou soer\n",
            "Output 2: e ma ri te se lyeu d@ se d@ le lza soer\n",
            "Traduction Output 1: c est jour à venger la douceur\n",
            "Traduction Output 2: et ma téméritélieu de ces de les soeur\n",
            "38695\n",
            "Input 1:  kail san d@ man de voulz va trai tr@ lai s@@ mwa\n",
            "Input 2:  le jywifs na tan d@@ ryin dun me shan tail k@ twa\n",
            "Output 1: e la ain e kou te de veu sai t@@ vair tu\n",
            "Output 2: ki s@ ki t@@ lai lai k@ j@ vyin drai pe tu\n",
            "Traduction Output 1: et la haine écoutez des voeux cette vertu\n",
            "Traduction Output 2: qui se quitte l est là que je s pétu\n",
            "1629\n",
            "Input 1:  ai sou van pour l@ syin ki te vo tr@ sair vis\n",
            "Input 2:  sai par la k@ ja vai me ri te mon su plis\n",
            "Output 1: sai s@ k@ mon lna mour m@ r@ fu z@@ d@ vou\n",
            "Output 2: e j@ sai ki m@ dwa k@ jai kou ron pour pou\n",
            "Traduction Output 1: c est ce que mon amour me refuse de vous\n",
            "Traduction Output 2: et je sais qui me doit que j ai couronne pour époux\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSsvkfYOUYlk",
        "colab_type": "code",
        "outputId": "8866f5a9-0004-43fe-e5b4-99dfc9030328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "text = []\n",
        "for i in range(5):\n",
        "    verse3, verse4 = generate_verses(verse3, verse4, temperature=temp, top_k=top_k, top_p=top_p)\n",
        "    print(verse3)\n",
        "    print(verse4)\n",
        "    text.append(translate(verse3, top_p=top_p2, temp=temp2, verbose=False))\n",
        "    text.append(translate(verse4, top_p=top_p2, temp=temp2, verbose=False))\n",
        "print()\n",
        "for s in text:\n",
        "    print(s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e j@ vou lzaa sair ve d@ lywi di fe ni e\n",
            "j@ n@ pywi vou re soudr a vaik un pair se e\n",
            "mai d@ sai t@@ gran koer n@ vou de nye sur mwa\n",
            "sai nyoer si vou l@ vwa pour vou a me de fwa\n",
            "si vou n@ vou lzan re k@ vou pou ve re nye\n",
            "e s@ rai vou de fan d@ kon di ny@@ de nye\n",
            "vou pou ve vou par tir d@ plu di ny@ d@ graas\n",
            "par de ja mai tou jour d@ ma pair dr@ l@ nas\n",
            "par le j@ pywi vou soel d@ sa vwar mon lna mour\n",
            "e me blou i no me n@ la peult a prai jour\n",
            "\n",
            "et je vous a de lui félicité\n",
            "je ne puis vous résoudre avec un père \n",
            "mais de cette grand coeur ne vous daigniez sur moi\n",
            "seigneur si vous le voit pour vous à mes démfois\n",
            "si vous ne vous en verrez que vous pouvez régner\n",
            "et serait vous défend de digne dédaigner\n",
            "vous pouvez vous partir de plus digne de grâce\n",
            "pardonnez mais toujours de ma perdre le menace\n",
            " / je puis vous seul de savoir mon amour\n",
            "et m ant ne la peut après jour\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-xyyWIBaawc",
        "colab_type": "text"
      },
      "source": [
        "et qu il ait sa constance et l amour qui le cours  \n",
        "on ne croit pas mon coeur avec ces contre jours  \n",
        "\n",
        "vous ne régnez son pas assez faire défendre  \n",
        "mais que la fin de même où la faire m entendre\n",
        "\n",
        "que pour un roi ma soeur par vos intelligence  \n",
        "si je n ai point de vous et dans une assurance\n",
        "\n",
        "mais ce n est pas un coeur que l amour vous égale  \n",
        "osez vous arrêter son hymen de rivale\n",
        "\n",
        "et l attendant ses efforts ce coeur qui dans mes yeux  \n",
        "qui n attentat rien contre leur confus furieux\n",
        "\n",
        "mais ma gloire de gloire et cela de mes yeux  \n",
        "et quand les ce moment ne serait pas des dieux\n",
        "\n",
        "seigneur à ce discours me fait voir son visage  \n",
        "avec elle est à vous pour régner son courage\n",
        "\n",
        "et le dit si je porte en ses telles conquêtes  \n",
        "et j ai vu le pouvoir que mes mains de sa tête\n",
        "\n",
        "et ne me saurait point que la mort de sa mère  \n",
        "mais pour me faire voir que je ne puis sans père\n",
        "\n",
        "hé seigneur de ces lieux je verrai me prétendre  \n",
        "et si je ne puis rien je ne me puis comprendre\n",
        "\n",
        "et pour un grand courage il est encor ce jour  \n",
        "pour vous voir avec moi de votre juste amour"
      ]
    }
  ]
}